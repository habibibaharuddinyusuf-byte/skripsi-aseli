{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”¬ Klasifikasi Penyakit Kulit Wajah dengan Deep Learning\n",
    "## Menggunakan CNN dan Transfer Learning\n",
    "\n",
    "Notebook ini mengimplementasikan sistem klasifikasi penyakit kulit wajah menggunakan:\n",
    "- **Transfer Learning** dengan MobileNetV2/ResNet50/VGG16\n",
    "- **Data Augmentation** untuk meningkatkan generalisasi\n",
    "- **Advanced Training Techniques** (callbacks, learning rate scheduling)\n",
    "- **Comprehensive Evaluation Metrics**\n",
    "\n",
    "**Asumsi:** Gambar input sudah fokus pada area wajah/penyakit kulit\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ 1. Setup dan Instalasi Library\n",
    "\n",
    "Instalasi dan import semua library yang diperlukan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalasi library (uncomment jika diperlukan)\n",
    "# !pip install tensorflow\n",
    "# !pip install scikit-learn\n",
    "# !pip install matplotlib\n",
    "# !pip install seaborn\n",
    "# !pip install pillow\n",
    "# !pip install numpy\n",
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import random\n",
    "from collections import Counter\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# TensorFlow dan Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Dropout, GlobalAveragePooling2D, \n",
    "    BatchNormalization, Activation, Input\n",
    ")\n",
    "from tensorflow.keras.applications import (\n",
    "    MobileNetV2, ResNet50, VGG16, EfficientNetB0\n",
    ")\n",
    "from tensorflow.keras.preprocessing.image import (\n",
    "    ImageDataGenerator, load_img, img_to_array\n",
    ")\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping, ReduceLROnPlateau, \n",
    "    ModelCheckpoint, TensorBoard\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Scikit-learn untuk evaluasi\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    roc_curve, auc, roc_auc_score\n",
    ")\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display versions\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU'))} GPU(s)\")\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(f\"GPU Device: {tf.config.list_physical_devices('GPU')[0].name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds untuk reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Konfigurasi plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Set TensorFlow memory growth (untuk GPU)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"âœ… GPU memory growth enabled\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"âš ï¸ {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ 2. Konfigurasi Parameter\n",
    "\n",
    "Definisikan semua hyperparameter dan path dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# KONFIGURASI PATH DATASET\n",
    "# ========================================\n",
    "\n",
    "# Untuk Google Colab: mount Google Drive terlebih dahulu\n",
    "USE_GOOGLE_DRIVE = False  # Set True jika menggunakan Google Drive\n",
    "\n",
    "if USE_GOOGLE_DRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DATASET_PATH = '/content/drive/MyDrive/skin_disease_dataset'\n",
    "else:\n",
    "    # Untuk lokal/Jupyter\n",
    "    DATASET_PATH = './dataset/augmented'\n",
    "\n",
    "# Struktur folder dataset yang diharapkan:\n",
    "# skin_disease_dataset/\n",
    "#   â”œâ”€â”€ Healthy/\n",
    "#   â”‚   â”œâ”€â”€ img1.jpg\n",
    "#   â”‚   â””â”€â”€ img2.jpg\n",
    "#   â”œâ”€â”€ Acne_Mild/\n",
    "#   â”œâ”€â”€ Acne_Severe/\n",
    "#   â”œâ”€â”€ Eczema/\n",
    "#   â”œâ”€â”€ Rosacea/\n",
    "#   â”œâ”€â”€ Melasma/\n",
    "#   â”œâ”€â”€ Psoriasis/\n",
    "#   â”œâ”€â”€ Vitiligo/\n",
    "#   â”œâ”€â”€ Seborrheic_Dermatitis/\n",
    "#   â””â”€â”€ Contact_Dermatitis/\n",
    "\n",
    "# Path untuk menyimpan model dan hasil\n",
    "OUTPUT_DIR = './output_training'\n",
    "MODEL_SAVE_PATH = os.path.join(OUTPUT_DIR, 'best_model.h5')\n",
    "HISTORY_SAVE_PATH = os.path.join(OUTPUT_DIR, 'training_history.json')\n",
    "LOG_DIR = os.path.join(OUTPUT_DIR, 'logs')\n",
    "\n",
    "# Buat direktori jika belum ada\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"ðŸ“ Dataset path: {DATASET_PATH}\")\n",
    "print(f\"ðŸ’¾ Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# HYPERPARAMETER\n",
    "# ========================================\n",
    "\n",
    "# Model configuration\n",
    "BASE_MODEL = 'MobileNetV2'  # Options: 'MobileNetV2', 'ResNet50', 'VGG16', 'EfficientNetB0'\n",
    "IMG_SIZE = (224, 224)  # Input size untuk model\n",
    "IMG_SHAPE = (*IMG_SIZE, 3)\n",
    "\n",
    "# Training parameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.0001\n",
    "VALIDATION_SPLIT = 0.2\n",
    "TEST_SPLIT = 0.1\n",
    "\n",
    "# Data augmentation parameters\n",
    "USE_AUGMENTATION = True\n",
    "AUGMENTATION_STRENGTH = 'medium'  # 'light', 'medium', 'heavy'\n",
    "\n",
    "# Model training strategy\n",
    "FINE_TUNE = True  # Jika True, akan unfreeze beberapa layer terakhir\n",
    "FINE_TUNE_AT = 100  # Layer ke-berapa mulai di-unfreeze\n",
    "\n",
    "# Class weights (untuk handle imbalanced dataset)\n",
    "USE_CLASS_WEIGHTS = True\n",
    "\n",
    "print(\"âœ… Hyperparameters configured:\")\n",
    "print(f\"   Base Model: {BASE_MODEL}\")\n",
    "print(f\"   Image Size: {IMG_SIZE}\")\n",
    "print(f\"   Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"   Epochs: {EPOCHS}\")\n",
    "print(f\"   Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"   Augmentation: {USE_AUGMENTATION}\")\n",
    "print(f\"   Fine-tuning: {FINE_TUNE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š 3. Data Loading dan Exploratory Data Analysis\n",
    "\n",
    "Load dataset dan analisis distribusi data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_path):\n",
    "    \"\"\"\n",
    "    Load dataset dari struktur folder.\n",
    "    \n",
    "    Returns:\n",
    "        images: List of image arrays\n",
    "        labels: List of label indices\n",
    "        class_names: List of class names\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    class_names = []\n",
    "    \n",
    "    # Validasi path\n",
    "    if not os.path.exists(dataset_path):\n",
    "        print(f\"âŒ Dataset path tidak ditemukan: {dataset_path}\")\n",
    "        print(\"\\nðŸ’¡ Pastikan struktur folder sesuai:\")\n",
    "        print(\"   skin_disease_dataset/\")\n",
    "        print(\"     â”œâ”€â”€ Healthy/\")\n",
    "        print(\"     â”œâ”€â”€ Acne_Mild/\")\n",
    "        print(\"     â””â”€â”€ ...\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Dapatkan semua kelas (subfolder)\n",
    "    class_folders = sorted([f for f in os.listdir(dataset_path) \n",
    "                           if os.path.isdir(os.path.join(dataset_path, f))])\n",
    "    \n",
    "    if len(class_folders) == 0:\n",
    "        print(\"âŒ Tidak ada folder kelas ditemukan\")\n",
    "        return None, None, None\n",
    "    \n",
    "    print(f\"\\nðŸ“‚ Ditemukan {len(class_folders)} kelas:\")\n",
    "    for idx, cls in enumerate(class_folders):\n",
    "        print(f\"   {idx}: {cls}\")\n",
    "    \n",
    "    print(\"\\nðŸ”„ Loading images...\")\n",
    "    \n",
    "    # Load images\n",
    "    for class_idx, class_name in enumerate(class_folders):\n",
    "        class_path = os.path.join(dataset_path, class_name)\n",
    "        class_names.append(class_name)\n",
    "        \n",
    "        # Dapatkan semua gambar\n",
    "        image_files = [f for f in os.listdir(class_path)\n",
    "                      if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
    "        \n",
    "        print(f\"   Loading {class_name}: {len(image_files)} images\", end='')\n",
    "        \n",
    "        loaded = 0\n",
    "        for img_file in image_files:\n",
    "            img_path = os.path.join(class_path, img_file)\n",
    "            \n",
    "            try:\n",
    "                # Load dan resize image\n",
    "                img = load_img(img_path, target_size=IMG_SIZE)\n",
    "                img_array = img_to_array(img)\n",
    "                \n",
    "                images.append(img_array)\n",
    "                labels.append(class_idx)\n",
    "                loaded += 1\n",
    "            except Exception as e:\n",
    "                print(f\"\\n   âš ï¸ Error loading {img_file}: {e}\")\n",
    "        \n",
    "        print(f\" â†’ âœ… {loaded} loaded\")\n",
    "    \n",
    "    print(f\"\\nâœ… Total loaded: {len(images)} images from {len(class_names)} classes\")\n",
    "    \n",
    "    return np.array(images), np.array(labels), class_names\n",
    "\n",
    "# Load dataset\n",
    "X, y, class_names = load_dataset(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validasi dataset\n",
    "if X is None or len(X) == 0:\n",
    "    print(\"\\nâŒ Dataset tidak dapat dimuat. Silakan periksa path dan struktur folder.\")\n",
    "    print(\"\\nðŸ“ Untuk membuat dataset dummy untuk testing:\")\n",
    "    print(\"   1. Buat folder: skin_disease_dataset/\")\n",
    "    print(\"   2. Buat subfolder untuk setiap kelas\")\n",
    "    print(\"   3. Masukkan gambar ke dalam folder yang sesuai\")\n",
    "else:\n",
    "    print(\"\\nðŸ“Š DATASET SUMMARY:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"  Total samples: {len(X)}\")\n",
    "    print(f\"  Image shape: {X[0].shape}\")\n",
    "    print(f\"  Number of classes: {len(class_names)}\")\n",
    "    print(f\"  Classes: {class_names}\")\n",
    "    print(f\"  Data type: {X.dtype}\")\n",
    "    print(f\"  Value range: [{X.min():.2f}, {X.max():.2f}]\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisasi distribusi kelas\n",
    "if X is not None and len(X) > 0:\n",
    "    label_counts = Counter(y)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "    \n",
    "    # Bar chart\n",
    "    ax1 = axes[0]\n",
    "    counts = [label_counts[i] for i in range(len(class_names))]\n",
    "    bars = ax1.bar(class_names, counts, color='skyblue', edgecolor='navy', alpha=0.7)\n",
    "    ax1.set_xlabel('Kelas Penyakit Kulit', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('Jumlah Sampel', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title('Distribusi Kelas Dataset', fontsize=14, fontweight='bold')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Tambahkan nilai di atas bar\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{int(height)}',\n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Pie chart\n",
    "    ax2 = axes[1]\n",
    "    colors = plt.cm.Set3(range(len(class_names)))\n",
    "    ax2.pie(counts, labels=class_names, autopct='%1.1f%%', startangle=90,\n",
    "            colors=colors)\n",
    "    ax2.set_title('Proporsi Kelas Dataset', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'class_distribution.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Check class imbalance\n",
    "    max_count = max(counts)\n",
    "    min_count = min(counts)\n",
    "    imbalance_ratio = max_count / min_count\n",
    "    \n",
    "    print(f\"\\nâš–ï¸ CLASS BALANCE ANALYSIS:\")\n",
    "    print(f\"   Max samples: {max_count}\")\n",
    "    print(f\"   Min samples: {min_count}\")\n",
    "    print(f\"   Imbalance ratio: {imbalance_ratio:.2f}\")\n",
    "    \n",
    "    if imbalance_ratio > 3:\n",
    "        print(f\"   âš ï¸ Dataset significantly imbalanced! Consider using class weights.\")\n",
    "    else:\n",
    "        print(f\"   âœ… Dataset reasonably balanced.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisasi sample images dari setiap kelas\n",
    "if X is not None and len(X) > 0:\n",
    "    n_classes = len(class_names)\n",
    "    n_samples_per_class = 5\n",
    "    \n",
    "    fig, axes = plt.subplots(n_classes, n_samples_per_class, \n",
    "                             figsize=(15, 3 * n_classes))\n",
    "    \n",
    "    for class_idx in range(n_classes):\n",
    "        # Get indices untuk kelas ini\n",
    "        class_indices = np.where(y == class_idx)[0]\n",
    "        \n",
    "        # Pilih random samples\n",
    "        if len(class_indices) >= n_samples_per_class:\n",
    "            sample_indices = np.random.choice(class_indices, n_samples_per_class, replace=False)\n",
    "        else:\n",
    "            sample_indices = class_indices\n",
    "        \n",
    "        for sample_idx, img_idx in enumerate(sample_indices[:n_samples_per_class]):\n",
    "            if n_classes > 1:\n",
    "                ax = axes[class_idx, sample_idx]\n",
    "            else:\n",
    "                ax = axes[sample_idx]\n",
    "            \n",
    "            # Display image\n",
    "            img = X[img_idx].astype('uint8')\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "            \n",
    "            # Label untuk kolom pertama\n",
    "            if sample_idx == 0:\n",
    "                ax.set_ylabel(class_names[class_idx], \n",
    "                            fontsize=11, fontweight='bold', rotation=0, \n",
    "                            ha='right', va='center')\n",
    "    \n",
    "    plt.suptitle('Sample Gambar dari Setiap Kelas', \n",
    "                fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'sample_images.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”„ 4. Data Preprocessing dan Augmentation\n",
    "\n",
    "Preprocessing data dan setup data augmentation untuk training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisasi data\n",
    "if X is not None and len(X) > 0:\n",
    "    print(\"ðŸ”„ Normalizing data...\")\n",
    "    \n",
    "    # Normalisasi ke [0, 1]\n",
    "    X_normalized = X.astype('float32') / 255.0\n",
    "    \n",
    "    print(f\"âœ… Data normalized\")\n",
    "    print(f\"   Original range: [{X.min()}, {X.max()}]\")\n",
    "    print(f\"   Normalized range: [{X_normalized.min():.3f}, {X_normalized.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset: Train, Validation, Test\n",
    "if X is not None and len(X) > 0:\n",
    "    print(\"\\nðŸ“Š Splitting dataset...\")\n",
    "    \n",
    "    # Split train and temp (val + test)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X_normalized, y,\n",
    "        test_size=(VALIDATION_SPLIT + TEST_SPLIT),\n",
    "        random_state=SEED,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    # Split temp into validation and test\n",
    "    val_test_ratio = TEST_SPLIT / (VALIDATION_SPLIT + TEST_SPLIT)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp,\n",
    "        test_size=val_test_ratio,\n",
    "        random_state=SEED,\n",
    "        stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    print(\"\\nâœ… Dataset split completed:\")\n",
    "    print(f\"   Training: {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "    print(f\"   Validation: {len(X_val)} samples ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "    print(f\"   Test: {len(X_test)} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "    print(f\"   Total: {len(X)} samples\")\n",
    "    \n",
    "    # Convert labels to categorical (one-hot encoding)\n",
    "    num_classes = len(class_names)\n",
    "    y_train_cat = to_categorical(y_train, num_classes)\n",
    "    y_val_cat = to_categorical(y_val, num_classes)\n",
    "    y_test_cat = to_categorical(y_test, num_classes)\n",
    "    \n",
    "    print(f\"\\nâœ… Labels converted to categorical\")\n",
    "    print(f\"   Shape: {y_train_cat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Data Augmentation\n",
    "if X is not None and len(X) > 0 and USE_AUGMENTATION:\n",
    "    print(\"\\nðŸŽ¨ Configuring data augmentation...\")\n",
    "    \n",
    "    if AUGMENTATION_STRENGTH == 'light':\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            rotation_range=10,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            horizontal_flip=True,\n",
    "            zoom_range=0.1,\n",
    "            fill_mode='nearest'\n",
    "        )\n",
    "    elif AUGMENTATION_STRENGTH == 'medium':\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            zoom_range=0.2,\n",
    "            shear_range=0.15,\n",
    "            brightness_range=[0.8, 1.2],\n",
    "            fill_mode='nearest'\n",
    "        )\n",
    "    else:  # heavy\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            rotation_range=30,\n",
    "            width_shift_range=0.3,\n",
    "            height_shift_range=0.3,\n",
    "            horizontal_flip=True,\n",
    "            vertical_flip=True,\n",
    "            zoom_range=0.3,\n",
    "            shear_range=0.2,\n",
    "            brightness_range=[0.7, 1.3],\n",
    "            fill_mode='nearest'\n",
    "        )\n",
    "    \n",
    "    # Validation data (no augmentation)\n",
    "    val_datagen = ImageDataGenerator()\n",
    "    \n",
    "    print(f\"âœ… Data augmentation configured ({AUGMENTATION_STRENGTH})\")\n",
    "    print(f\"   Rotation: {train_datagen.rotation_range}Â°\")\n",
    "    print(f\"   Width shift: {train_datagen.width_shift_range}\")\n",
    "    print(f\"   Height shift: {train_datagen.height_shift_range}\")\n",
    "    print(f\"   Zoom: {train_datagen.zoom_range}\")\n",
    "    print(f\"   Horizontal flip: {train_datagen.horizontal_flip}\")\n",
    "else:\n",
    "    # No augmentation\n",
    "    train_datagen = ImageDataGenerator()\n",
    "    val_datagen = ImageDataGenerator()\n",
    "    print(\"â„¹ï¸ Data augmentation disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisasi augmented images (sample)\n",
    "if X is not None and len(X) > 0 and USE_AUGMENTATION:\n",
    "    print(\"\\nðŸŽ¨ Sample augmented images:\")\n",
    "    \n",
    "    # Pilih satu sample image\n",
    "    sample_img = X_train[0:1]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(sample_img[0])\n",
    "    axes[0].set_title('Original', fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Generate augmented versions\n",
    "    aug_iter = train_datagen.flow(sample_img, batch_size=1)\n",
    "    \n",
    "    for i in range(1, 10):\n",
    "        aug_img = next(aug_iter)[0]\n",
    "        axes[i].imshow(aug_img)\n",
    "        axes[i].set_title(f'Augmented {i}', fontweight='bold')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Data Augmentation Examples', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'augmentation_samples.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights untuk handle imbalanced dataset\n",
    "if X is not None and len(X) > 0 and USE_CLASS_WEIGHTS:\n",
    "    print(\"\\nâš–ï¸ Calculating class weights...\")\n",
    "    \n",
    "    from sklearn.utils import class_weight\n",
    "    \n",
    "    class_weights_array = class_weight.compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=np.unique(y_train),\n",
    "        y=y_train\n",
    "    )\n",
    "    \n",
    "    class_weights = dict(enumerate(class_weights_array))\n",
    "    \n",
    "    print(\"âœ… Class weights computed:\")\n",
    "    for cls_idx, weight in class_weights.items():\n",
    "        print(f\"   {class_names[cls_idx]}: {weight:.3f}\")\n",
    "else:\n",
    "    class_weights = None\n",
    "    print(\"â„¹ï¸ Class weights disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ 5. Model Building dengan Transfer Learning\n",
    "\n",
    "Membangun model CNN menggunakan pre-trained model sebagai feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(base_model_name='MobileNetV2', num_classes=10, \n",
    "                img_shape=(224, 224, 3), learning_rate=0.0001):\n",
    "    \"\"\"\n",
    "    Build CNN model dengan transfer learning.\n",
    "    \n",
    "    Args:\n",
    "        base_model_name: Nama base model ('MobileNetV2', 'ResNet50', 'VGG16', 'EfficientNetB0')\n",
    "        num_classes: Jumlah kelas output\n",
    "        img_shape: Shape input image\n",
    "        learning_rate: Learning rate untuk optimizer\n",
    "    \n",
    "    Returns:\n",
    "        model: Compiled Keras model\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸ—ï¸ Building model with {base_model_name}...\")\n",
    "    \n",
    "    # Load pre-trained base model\n",
    "    if base_model_name == 'MobileNetV2':\n",
    "        base_model = MobileNetV2(\n",
    "            input_shape=img_shape,\n",
    "            include_top=False,\n",
    "            weights='imagenet'\n",
    "        )\n",
    "    elif base_model_name == 'ResNet50':\n",
    "        base_model = ResNet50(\n",
    "            input_shape=img_shape,\n",
    "            include_top=False,\n",
    "            weights='imagenet'\n",
    "        )\n",
    "    elif base_model_name == 'VGG16':\n",
    "        base_model = VGG16(\n",
    "            input_shape=img_shape,\n",
    "            include_top=False,\n",
    "            weights='imagenet'\n",
    "        )\n",
    "    elif base_model_name == 'EfficientNetB0':\n",
    "        base_model = EfficientNetB0(\n",
    "            input_shape=img_shape,\n",
    "            include_top=False,\n",
    "            weights='imagenet'\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown base model: {base_model_name}\")\n",
    "    \n",
    "    # Freeze base model layers\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Build complete model\n",
    "    inputs = Input(shape=img_shape)\n",
    "    \n",
    "    # Base model\n",
    "    x = base_model(inputs, training=False)\n",
    "    \n",
    "    # Classification head\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs, name=f'{base_model_name}_SkinClassifier')\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            keras.metrics.Precision(name='precision'),\n",
    "            keras.metrics.Recall(name='recall'),\n",
    "            keras.metrics.AUC(name='auc')\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ… Model built successfully\")\n",
    "    print(f\"   Base model: {base_model_name}\")\n",
    "    print(f\"   Total layers: {len(model.layers)}\")\n",
    "    print(f\"   Trainable params: {model.count_params():,}\")\n",
    "    \n",
    "    return model, base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "if X is not None and len(X) > 0:\n",
    "    model, base_model = build_model(\n",
    "        base_model_name=BASE_MODEL,\n",
    "        num_classes=num_classes,\n",
    "        img_shape=IMG_SHAPE,\n",
    "        learning_rate=LEARNING_RATE\n",
    "    )\n",
    "    \n",
    "    # Display model summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MODEL SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisasi arsitektur model\n",
    "if X is not None and len(X) > 0:\n",
    "    try:\n",
    "        keras.utils.plot_model(\n",
    "            model,\n",
    "            to_file=os.path.join(OUTPUT_DIR, 'model_architecture.png'),\n",
    "            show_shapes=True,\n",
    "            show_layer_names=True,\n",
    "            rankdir='TB',\n",
    "            expand_nested=False,\n",
    "            dpi=96\n",
    "        )\n",
    "        print(\"âœ… Model architecture diagram saved\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Could not create model diagram: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ 6. Training Configuration dan Callbacks\n",
    "\n",
    "Setup callbacks untuk monitoring dan optimization training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup callbacks\n",
    "if X is not None and len(X) > 0:\n",
    "    print(\"\\nâš™ï¸ Configuring training callbacks...\")\n",
    "    \n",
    "    callbacks = [\n",
    "        # Early stopping\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1,\n",
    "            mode='min'\n",
    "        ),\n",
    "        \n",
    "        # Reduce learning rate on plateau\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=7,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1,\n",
    "            mode='min'\n",
    "        ),\n",
    "        \n",
    "        # Model checkpoint - save best model\n",
    "        ModelCheckpoint(\n",
    "            MODEL_SAVE_PATH,\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            verbose=1,\n",
    "            mode='max'\n",
    "        ),\n",
    "        \n",
    "        # TensorBoard logging\n",
    "        TensorBoard(\n",
    "            log_dir=LOG_DIR,\n",
    "            histogram_freq=1,\n",
    "            write_graph=True,\n",
    "            write_images=False,\n",
    "            update_freq='epoch'\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    print(\"âœ… Callbacks configured:\")\n",
    "    print(\"   - EarlyStopping (patience=15)\")\n",
    "    print(\"   - ReduceLROnPlateau (factor=0.5, patience=7)\")\n",
    "    print(\"   - ModelCheckpoint (save best model)\")\n",
    "    print(\"   - TensorBoard logging\")\n",
    "    print(f\"\\nðŸ’¡ To view TensorBoard: tensorboard --logdir={LOG_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ 7. Model Training\n",
    "\n",
    "Train model dengan data yang sudah diproses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Phase 1: Transfer Learning (frozen base)\n",
    "if X is not None and len(X) > 0:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸš€ PHASE 1: TRAINING WITH FROZEN BASE MODEL\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Base model: {BASE_MODEL} (frozen)\")\n",
    "    print(f\"Epochs: {EPOCHS}\")\n",
    "    print(f\"Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Validation samples: {len(X_val)}\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # Start training\n",
    "    history = model.fit(\n",
    "        train_datagen.flow(X_train, y_train_cat, batch_size=BATCH_SIZE),\n",
    "        validation_data=(X_val, y_val_cat),\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weights,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"âœ… PHASE 1 TRAINING COMPLETED\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Phase 2: Fine-tuning (unfreeze some layers)\n",
    "if X is not None and len(X) > 0 and FINE_TUNE:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸ”¥ PHASE 2: FINE-TUNING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Unfreeze base model layers\n",
    "    base_model.trainable = True\n",
    "    \n",
    "    # Freeze early layers, unfreeze later layers\n",
    "    for layer in base_model.layers[:FINE_TUNE_AT]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    print(f\"Base model layers: {len(base_model.layers)}\")\n",
    "    print(f\"Frozen layers: {FINE_TUNE_AT}\")\n",
    "    print(f\"Trainable layers: {len(base_model.layers) - FINE_TUNE_AT}\")\n",
    "    \n",
    "    # Recompile with lower learning rate\n",
    "    fine_tune_lr = LEARNING_RATE / 10\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=fine_tune_lr),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            keras.metrics.Precision(name='precision'),\n",
    "            keras.metrics.Recall(name='recall'),\n",
    "            keras.metrics.AUC(name='auc')\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(f\"Fine-tuning learning rate: {fine_tune_lr}\")\n",
    "    print(f\"Trainable params: {model.count_params():,}\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # Continue training\n",
    "    fine_tune_epochs = 20\n",
    "    total_epochs = len(history.history['loss']) + fine_tune_epochs\n",
    "    \n",
    "    history_fine = model.fit(\n",
    "        train_datagen.flow(X_train, y_train_cat, batch_size=BATCH_SIZE),\n",
    "        validation_data=(X_val, y_val_cat),\n",
    "        epochs=total_epochs,\n",
    "        initial_epoch=len(history.history['loss']),\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weights,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Merge histories\n",
    "    for key in history.history.keys():\n",
    "        history.history[key].extend(history_fine.history[key])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"âœ… PHASE 2 FINE-TUNING COMPLETED\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training history\n",
    "if X is not None and len(X) > 0:\n",
    "    print(\"\\nðŸ’¾ Saving training history...\")\n",
    "    \n",
    "    # Convert to serializable format\n",
    "    history_dict = {}\n",
    "    for key, value in history.history.items():\n",
    "        history_dict[key] = [float(v) for v in value]\n",
    "    \n",
    "    with open(HISTORY_SAVE_PATH, 'w') as f:\n",
    "        json.dump(history_dict, f, indent=2)\n",
    "    \n",
    "    print(f\"âœ… Training history saved to: {HISTORY_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š 8. Training Visualization\n",
    "\n",
    "Visualisasi training history untuk analisis performa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "if X is not None and len(X) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    epochs_range = range(1, len(history.history['accuracy']) + 1)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[0, 0].plot(epochs_range, history.history['accuracy'], \n",
    "                    label='Train Accuracy', linewidth=2, marker='o', markersize=4)\n",
    "    axes[0, 0].plot(epochs_range, history.history['val_accuracy'], \n",
    "                    label='Val Accuracy', linewidth=2, marker='s', markersize=4)\n",
    "    axes[0, 0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0, 0].set_ylabel('Accuracy', fontsize=12)\n",
    "    axes[0, 0].legend(loc='lower right', fontsize=10)\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss\n",
    "    axes[0, 1].plot(epochs_range, history.history['loss'], \n",
    "                    label='Train Loss', linewidth=2, marker='o', markersize=4)\n",
    "    axes[0, 1].plot(epochs_range, history.history['val_loss'], \n",
    "                    label='Val Loss', linewidth=2, marker='s', markersize=4)\n",
    "    axes[0, 1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0, 1].set_ylabel('Loss', fontsize=12)\n",
    "    axes[0, 1].legend(loc='upper right', fontsize=10)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Precision\n",
    "    axes[1, 0].plot(epochs_range, history.history['precision'], \n",
    "                    label='Train Precision', linewidth=2, marker='o', markersize=4)\n",
    "    axes[1, 0].plot(epochs_range, history.history['val_precision'], \n",
    "                    label='Val Precision', linewidth=2, marker='s', markersize=4)\n",
    "    axes[1, 0].set_title('Model Precision', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Precision', fontsize=12)\n",
    "    axes[1, 0].legend(loc='lower right', fontsize=10)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Recall\n",
    "    axes[1, 1].plot(epochs_range, history.history['recall'], \n",
    "                    label='Train Recall', linewidth=2, marker='o', markersize=4)\n",
    "    axes[1, 1].plot(epochs_range, history.history['val_recall'], \n",
    "                    label='Val Recall', linewidth=2, marker='s', markersize=4)\n",
    "    axes[1, 1].set_title('Model Recall', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1, 1].set_ylabel('Recall', fontsize=12)\n",
    "    axes[1, 1].legend(loc='lower right', fontsize=10)\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'training_history.png'), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ… Training history plots saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ 9. Model Evaluation pada Test Set\n",
    "\n",
    "Evaluasi performa model pada data test yang belum pernah dilihat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "if X is not None and len(X) > 0:\n",
    "    print(\"\\nðŸ“¥ Loading best model...\")\n",
    "    \n",
    "    if os.path.exists(MODEL_SAVE_PATH):\n",
    "        model = load_model(MODEL_SAVE_PATH)\n",
    "        print(f\"âœ… Best model loaded from: {MODEL_SAVE_PATH}\")\n",
    "    else:\n",
    "        print(\"âš ï¸ Best model file not found, using current model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "if X is not None and len(X) > 0:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸ” EVALUATING MODEL ON TEST SET\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_proba = model.predict(X_test, verbose=0)\n",
    "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "    \n",
    "    # Evaluate\n",
    "    test_results = model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "    \n",
    "    # Extract metrics\n",
    "    test_loss = test_results[0]\n",
    "    test_accuracy = test_results[1]\n",
    "    test_precision = test_results[2]\n",
    "    test_recall = test_results[3]\n",
    "    test_auc = test_results[4]\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_test, y_pred, average='weighted', zero_division=0\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nðŸ“Š TEST SET METRICS:\")\n",
    "    print(f\"   Loss: {test_loss:.4f}\")\n",
    "    print(f\"   Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "    print(f\"   Precision: {test_precision:.4f}\")\n",
    "    print(f\"   Recall: {test_recall:.4f}\")\n",
    "    print(f\"   F1-Score: {f1:.4f}\")\n",
    "    print(f\"   AUC: {test_auc:.4f}\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "if X is not None and len(X) > 0:\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.title('Confusion Matrix - Test Set', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'confusion_matrix.png'), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ… Confusion matrix saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "if X is not None and len(X) > 0:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸ“‹ CLASSIFICATION REPORT\")\n",
    "    print(\"=\"*70)\n",
    "    print(classification_report(y_test, y_pred, target_names=class_names, digits=4))\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class metrics visualization\n",
    "if X is not None and len(X) > 0:\n",
    "    # Calculate per-class accuracy\n",
    "    per_class_accuracy = []\n",
    "    per_class_precision = []\n",
    "    per_class_recall = []\n",
    "    per_class_f1 = []\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        mask = y_test == i\n",
    "        if mask.sum() > 0:\n",
    "            # Accuracy\n",
    "            acc = (y_pred[mask] == y_test[mask]).sum() / mask.sum()\n",
    "            per_class_accuracy.append(acc)\n",
    "            \n",
    "            # Other metrics\n",
    "            p, r, f, _ = precision_recall_fscore_support(\n",
    "                y_test[mask], y_pred[mask], average='binary', zero_division=0\n",
    "            )\n",
    "            per_class_precision.append(p)\n",
    "            per_class_recall.append(r)\n",
    "            per_class_f1.append(f)\n",
    "        else:\n",
    "            per_class_accuracy.append(0)\n",
    "            per_class_precision.append(0)\n",
    "            per_class_recall.append(0)\n",
    "            per_class_f1.append(0)\n",
    "    \n",
    "    # Plot per-class metrics\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    metrics = [\n",
    "        (per_class_accuracy, 'Per-Class Accuracy', axes[0, 0]),\n",
    "        (per_class_precision, 'Per-Class Precision', axes[0, 1]),\n",
    "        (per_class_recall, 'Per-Class Recall', axes[1, 0]),\n",
    "        (per_class_f1, 'Per-Class F1-Score', axes[1, 1])\n",
    "    ]\n",
    "    \n",
    "    for metric_values, title, ax in metrics:\n",
    "        bars = ax.bar(class_names, metric_values, color='lightcoral', \n",
    "                     edgecolor='darkred', alpha=0.7)\n",
    "        ax.axhline(y=np.mean(metric_values), color='blue', linestyle='--', \n",
    "                  linewidth=2, label=f'Mean: {np.mean(metric_values):.3f}')\n",
    "        ax.set_xlabel('Class', fontsize=11, fontweight='bold')\n",
    "        ax.set_ylabel('Score', fontsize=11, fontweight='bold')\n",
    "        ax.set_title(title, fontsize=13, fontweight='bold')\n",
    "        ax.set_xticklabels(class_names, rotation=45, ha='right')\n",
    "        ax.set_ylim([0, 1.1])\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.3f}',\n",
    "                   ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'per_class_metrics.png'), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ… Per-class metrics plots saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”® 10. Prediction Pipeline\n",
    "\n",
    "Implementasi fungsi untuk prediksi pada gambar baru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_skin_disease(image_path, model, class_names, img_size=(224, 224), visualize=True):\n",
    "    \"\"\"\n",
    "    Prediksi penyakit kulit dari gambar.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path ke gambar input\n",
    "        model: Model Keras untuk prediksi\n",
    "        class_names: List nama kelas\n",
    "        img_size: Ukuran input model\n",
    "        visualize: Jika True, tampilkan visualisasi\n",
    "    \n",
    "    Returns:\n",
    "        result: Dictionary berisi hasil prediksi\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'success': False,\n",
    "        'image_path': image_path,\n",
    "        'predicted_class': None,\n",
    "        'confidence': None,\n",
    "        'all_probabilities': None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Load image\n",
    "        img = load_img(image_path, target_size=img_size)\n",
    "        img_array = img_to_array(img)\n",
    "        \n",
    "        # Preprocess\n",
    "        img_normalized = img_array / 255.0\n",
    "        img_batch = np.expand_dims(img_normalized, axis=0)\n",
    "        \n",
    "        # Predict\n",
    "        predictions = model.predict(img_batch, verbose=0)[0]\n",
    "        predicted_class_idx = np.argmax(predictions)\n",
    "        predicted_class = class_names[predicted_class_idx]\n",
    "        confidence = predictions[predicted_class_idx]\n",
    "        \n",
    "        # Store results\n",
    "        result['success'] = True\n",
    "        result['predicted_class'] = predicted_class\n",
    "        result['confidence'] = float(confidence)\n",
    "        result['all_probabilities'] = {\n",
    "            class_names[i]: float(predictions[i]) \n",
    "            for i in range(len(class_names))\n",
    "        }\n",
    "        \n",
    "        # Visualize\n",
    "        if visualize:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "            \n",
    "            # Image\n",
    "            axes[0].imshow(img)\n",
    "            axes[0].set_title(f'Input Image', fontsize=13, fontweight='bold')\n",
    "            axes[0].axis('off')\n",
    "            \n",
    "            # Prediction probabilities\n",
    "            sorted_indices = np.argsort(predictions)[::-1]\n",
    "            sorted_classes = [class_names[i] for i in sorted_indices]\n",
    "            sorted_probs = [predictions[i] for i in sorted_indices]\n",
    "            \n",
    "            y_pos = np.arange(len(class_names))\n",
    "            colors = ['green' if i == predicted_class_idx else 'lightblue' \n",
    "                     for i in sorted_indices]\n",
    "            \n",
    "            axes[1].barh(y_pos, sorted_probs, color=colors, edgecolor='navy', alpha=0.7)\n",
    "            axes[1].set_yticks(y_pos)\n",
    "            axes[1].set_yticklabels(sorted_classes)\n",
    "            axes[1].set_xlabel('Probability', fontsize=11, fontweight='bold')\n",
    "            axes[1].set_title(\n",
    "                f'Prediction: {predicted_class}\\nConfidence: {confidence:.2%}',\n",
    "                fontsize=13, fontweight='bold'\n",
    "            )\n",
    "            axes[1].set_xlim([0, 1])\n",
    "            axes[1].grid(True, alpha=0.3, axis='x')\n",
    "            \n",
    "            # Add percentage labels\n",
    "            for i, (prob, color) in enumerate(zip(sorted_probs, colors)):\n",
    "                axes[1].text(prob + 0.02, i, f'{prob:.1%}',\n",
    "                           va='center', fontweight='bold', fontsize=9)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error predicting {image_path}: {e}\")\n",
    "        return result\n",
    "\n",
    "print(\"âœ… Prediction function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prediction pada sample dari test set\n",
    "if X is not None and len(X) > 0:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸ”® TESTING PREDICTION ON SAMPLE IMAGES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Pilih beberapa random samples\n",
    "    n_samples = min(3, len(X_test))\n",
    "    sample_indices = np.random.choice(len(X_test), n_samples, replace=False)\n",
    "    \n",
    "    for idx in sample_indices:\n",
    "        print(f\"\\n--- Sample {idx+1} ---\")\n",
    "        \n",
    "        # Save sample to temp file\n",
    "        temp_img_path = os.path.join(OUTPUT_DIR, f'temp_sample_{idx}.jpg')\n",
    "        sample_img = (X_test[idx] * 255).astype(np.uint8)\n",
    "        Image.fromarray(sample_img).save(temp_img_path)\n",
    "        \n",
    "        true_label = class_names[y_test[idx]]\n",
    "        print(f\"True Label: {true_label}\")\n",
    "        \n",
    "        # Predict\n",
    "        result = predict_skin_disease(temp_img_path, model, class_names, \n",
    "                                      img_size=IMG_SIZE, visualize=(idx == sample_indices[0]))\n",
    "        \n",
    "        if result['success']:\n",
    "            pred_label = result['predicted_class']\n",
    "            confidence = result['confidence']\n",
    "            \n",
    "            status = \"âœ… Correct\" if pred_label == true_label else \"âŒ Wrong\"\n",
    "            print(f\"Predicted: {pred_label} (Confidence: {confidence:.2%}) {status}\")\n",
    "        \n",
    "        # Cleanup temp file\n",
    "        if os.path.exists(temp_img_path):\n",
    "            os.remove(temp_img_path)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction pada Gambar Baru (Custom Input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediksi pada gambar baru dari user\n",
    "# Uncomment dan set path untuk melakukan prediksi\n",
    "\n",
    "# NEW_IMAGE_PATH = './path/to/your/new/image.jpg'\n",
    "# \n",
    "# if os.path.exists(NEW_IMAGE_PATH) and X is not None:\n",
    "#     print(\"ðŸ”® Melakukan prediksi pada gambar baru...\\n\")\n",
    "#     result = predict_skin_disease(NEW_IMAGE_PATH, model, class_names, \n",
    "#                                   img_size=IMG_SIZE, visualize=True)\n",
    "#     \n",
    "#     if result['success']:\n",
    "#         print(\"\\nðŸ“Š HASIL PREDIKSI:\")\n",
    "#         print(f\"   Penyakit: {result['predicted_class']}\")\n",
    "#         print(f\"   Confidence: {result['confidence']:.2%}\")\n",
    "#         print(\"\\n   Top 3 Predictions:\")\n",
    "#         sorted_probs = sorted(result['all_probabilities'].items(), \n",
    "#                              key=lambda x: x[1], reverse=True)[:3]\n",
    "#         for i, (cls, prob) in enumerate(sorted_probs, 1):\n",
    "#             print(f\"     {i}. {cls}: {prob:.2%}\")\n",
    "#     else:\n",
    "#         print(\"âŒ Prediksi gagal\")\n",
    "# else:\n",
    "#     print(\"âš ï¸ Set NEW_IMAGE_PATH untuk melakukan prediksi pada gambar baru\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¾ 11. Save Model dan Metadata\n",
    "\n",
    "Simpan model dan informasi penting untuk deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "if X is not None and len(X) > 0:\n",
    "    print(\"\\nðŸ’¾ Saving final model and metadata...\")\n",
    "    \n",
    "    # Save model\n",
    "    final_model_path = os.path.join(OUTPUT_DIR, 'skin_disease_classifier_final.h5')\n",
    "    model.save(final_model_path)\n",
    "    print(f\"âœ… Model saved: {final_model_path}\")\n",
    "    \n",
    "    # Save class names\n",
    "    class_names_path = os.path.join(OUTPUT_DIR, 'class_names.json')\n",
    "    with open(class_names_path, 'w') as f:\n",
    "        json.dump(class_names, f, indent=2)\n",
    "    print(f\"âœ… Class names saved: {class_names_path}\")\n",
    "    \n",
    "    # Save model metadata\n",
    "    metadata = {\n",
    "        'model_name': f'{BASE_MODEL}_SkinClassifier',\n",
    "        'base_model': BASE_MODEL,\n",
    "        'num_classes': num_classes,\n",
    "        'class_names': class_names,\n",
    "        'img_size': IMG_SIZE,\n",
    "        'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'total_samples': len(X),\n",
    "        'train_samples': len(X_train),\n",
    "        'val_samples': len(X_val),\n",
    "        'test_samples': len(X_test),\n",
    "        'test_accuracy': float(test_accuracy),\n",
    "        'test_precision': float(test_precision),\n",
    "        'test_recall': float(test_recall),\n",
    "        'test_f1': float(f1),\n",
    "        'test_auc': float(test_auc),\n",
    "        'hyperparameters': {\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'epochs': EPOCHS,\n",
    "            'learning_rate': LEARNING_RATE,\n",
    "            'use_augmentation': USE_AUGMENTATION,\n",
    "            'augmentation_strength': AUGMENTATION_STRENGTH,\n",
    "            'fine_tune': FINE_TUNE,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    metadata_path = os.path.join(OUTPUT_DIR, 'model_metadata.json')\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"âœ… Metadata saved: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ 12. Summary dan Kesimpulan\n",
    "\n",
    "Ringkasan hasil training dan panduan deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X is not None and len(X) > 0:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸ“Š PROJECT SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Model: {BASE_MODEL} Skin Disease Classifier\")\n",
    "    \n",
    "    print(f\"\\nðŸ“‚ Dataset:\")\n",
    "    print(f\"   Total samples: {len(X)}\")\n",
    "    print(f\"   Classes: {len(class_names)}\")\n",
    "    print(f\"   Class names: {', '.join(class_names)}\")\n",
    "    print(f\"   Split: {len(X_train)} train / {len(X_val)} val / {len(X_test)} test\")\n",
    "    \n",
    "    print(f\"\\nðŸ—ï¸ Model Architecture:\")\n",
    "    print(f\"   Base model: {BASE_MODEL}\")\n",
    "    print(f\"   Input size: {IMG_SIZE}\")\n",
    "    print(f\"   Total parameters: {model.count_params():,}\")\n",
    "    print(f\"   Fine-tuning: {'Yes' if FINE_TUNE else 'No'}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ Performance Metrics (Test Set):\")\n",
    "    print(f\"   Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "    print(f\"   Precision: {test_precision:.4f}\")\n",
    "    print(f\"   Recall: {test_recall:.4f}\")\n",
    "    print(f\"   F1-Score: {f1:.4f}\")\n",
    "    print(f\"   AUC: {test_auc:.4f}\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¾ Output Files:\")\n",
    "    print(f\"   Model: {final_model_path}\")\n",
    "    print(f\"   Best model: {MODEL_SAVE_PATH}\")\n",
    "    print(f\"   Class names: {class_names_path}\")\n",
    "    print(f\"   Metadata: {metadata_path}\")\n",
    "    print(f\"   Training history: {HISTORY_SAVE_PATH}\")\n",
    "    print(f\"   Visualizations: {OUTPUT_DIR}/*.png\")\n",
    "    \n",
    "    print(f\"\\nðŸš€ Deployment:\")\n",
    "    print(f\"   1. Load model: model = load_model('{final_model_path}')\")\n",
    "    print(f\"   2. Load classes: with open('{class_names_path}') as f: classes = json.load(f)\")\n",
    "    print(f\"   3. Predict: predict_skin_disease(image_path, model, classes)\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¡ Next Steps:\")\n",
    "    print(f\"   - Deploy model as REST API (Flask/FastAPI)\")\n",
    "    print(f\"   - Convert to TensorFlow Lite for mobile\")\n",
    "    print(f\"   - Create web interface\")\n",
    "    print(f\"   - Collect more data for improvement\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"âœ… PROJECT COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"âš ï¸ PROJECT INCOMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nDataset tidak ditemukan atau tidak valid.\")\n",
    "    print(\"\\nSilakan:\")\n",
    "    print(\"1. Periksa DATASET_PATH\")\n",
    "    print(\"2. Pastikan struktur folder benar\")\n",
    "    print(\"3. Jalankan ulang notebook\")\n",
    "    print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ“ User Guide\n",
    "\n",
    "### Quick Start:\n",
    "\n",
    "1. **Setup Dataset:**\n",
    "   ```\n",
    "   skin_disease_dataset/\n",
    "   â”œâ”€â”€ Healthy/\n",
    "   â”œâ”€â”€ Acne_Mild/\n",
    "   â”œâ”€â”€ Acne_Severe/\n",
    "   â””â”€â”€ ...\n",
    "   ```\n",
    "\n",
    "2. **Configure Parameters:** Edit section 2 (Konfigurasi Parameter)\n",
    "\n",
    "3. **Run All Cells:** Cell â†’ Run All\n",
    "\n",
    "4. **Monitor Training:** Check TensorBoard or training plots\n",
    "\n",
    "5. **Evaluate Results:** Check test metrics and confusion matrix\n",
    "\n",
    "6. **Deploy:** Use saved model for predictions\n",
    "\n",
    "### Tips:\n",
    "\n",
    "- **GPU Acceleration:** Enable GPU in Colab (Runtime â†’ Change runtime type â†’ GPU)\n",
    "- **Small Dataset:** Use heavy augmentation and smaller batch size\n",
    "- **Overfitting:** Increase dropout or add more regularization\n",
    "- **Low Accuracy:** Try different base models or increase epochs\n",
    "- **Imbalanced Data:** Use class weights or SMOTE\n",
    "\n",
    "### Model Selection:\n",
    "\n",
    "- **MobileNetV2:** Fast, lightweight (best for mobile)\n",
    "- **ResNet50:** Accurate, medium speed\n",
    "- **VGG16:** Simple architecture, slower\n",
    "- **EfficientNetB0:** Best accuracy/efficiency trade-off\n",
    "\n",
    "### Deployment Options:\n",
    "\n",
    "1. **REST API:** Flask/FastAPI web service\n",
    "2. **Mobile App:** TensorFlow Lite conversion\n",
    "3. **Web App:** TensorFlow.js for browser\n",
    "4. **Cloud:** Deploy to AWS/GCP/Azure\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ“§ Support:** For issues or questions, refer to TensorFlow documentation or open an issue.\n",
    "\n",
    "**Happy Training! ðŸš€**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
