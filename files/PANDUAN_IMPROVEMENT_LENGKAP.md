# üìö PANDUAN LENGKAP: Meningkatkan Akurasi dengan Dataset Kecil

## üéØ Executive Summary

**Situasi Anda:**
- 10 kelas penyakit kulit
- Hanya 11 foto per kelas = **110 foto total**
- Foto **TIDAK KONSISTEN** (berbagai angle, fokus, dll)

**Masalah Utama:**
1. ‚ùå Dataset TERLALU KECIL untuk deep learning
2. ‚ùå Inkonsistensi foto akan membingungkan model
3. ‚ùå Mudah sekali overfit
4. ‚ùå Tidak cukup variasi untuk generalisasi

**Solusi Implemented:**
‚úÖ Heavy Data Augmentation (15x multiplier)
‚úÖ K-Fold Cross Validation (5 folds)
‚úÖ Test-Time Augmentation (10 variants)
‚úÖ Ensemble Predictions
‚úÖ Advanced Regularization
‚úÖ Smaller Model Architecture

---

## üìä PERBANDINGAN: Original vs Improved

| Aspek | Original Code | Improved Code | Kenapa Penting? |
|-------|---------------|---------------|-----------------|
| **Image Size** | 128x128 | **96x96** | Reduce parameters, less overfitting |
| **Batch Size** | 32 | **4** | More weight updates, better for small data |
| **Augmentation** | Medium (rotation 20¬∞) | **Heavy (rotation 40¬∞)** | Generate lebih banyak variasi |
| **Augmentation Factor** | None | **15x (11‚Üí165 foto/kelas)** | Dramatically increase data |
| **Validation** | Simple train/val split | **K-Fold CV (5 folds)** | Every sample used for training AND validation |
| **Test Prediction** | Single forward pass | **TTA (10 augmentations averaged)** | Reduce prediction variance |
| **Ensemble** | Single model | **5 models (K-fold)** | More robust predictions |
| **Regularization** | Moderate dropout | **Heavy dropout + L2 + BatchNorm** | Prevent overfitting aggressively |
| **Dense Layers** | 512-256-128 | **256-128** | Fewer parameters = less overfitting |

---

## üé® STRATEGI AUGMENTATION UNTUK DATASET KECIL

### 1. **Heavy Augmentation Parameters**

```python
ImageDataGenerator(
    rotation_range=40,           # ‚≠ê Rotasi hingga ¬±40¬∞ (konsisten angle)
    width_shift_range=0.3,       # ‚≠ê Geser horizontal 30%
    height_shift_range=0.3,      # ‚≠ê Geser vertikal 30%
    shear_range=0.3,             # Shear transformation
    zoom_range=0.4,              # Zoom in/out 40%
    horizontal_flip=True,        # ‚≠ê Mirror horizontal
    vertical_flip=True,          # ‚≠ê Mirror vertical (OK untuk skin)
    brightness_range=[0.5, 1.5], # ‚≠ê Variasi pencahayaan
    channel_shift_range=30,      # Shift warna RGB
    fill_mode='reflect'          # Fill pixels yang kosong
)
```

**Kenapa Aggressive?**
- Dengan 11 foto, model akan HAFAL training data
- Aggressive augmentation membuat model "tidak bisa" hafal
- Force model untuk belajar fitur yang robust, bukan memorize

### 2. **Advanced Custom Augmentation**

Selain Keras ImageDataGenerator, tambahkan:

```python
def advanced_augment(image):
    # Random noise (simulate sensor noise)
    noise = np.random.normal(0, 5, image.shape)
    image = image + noise
    
    # Random contrast adjustment
    factor = np.random.uniform(0.7, 1.3)
    image = 128 + factor * (image - 128)
    
    # Random saturation (skin tone variation)
    hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
    hsv[:,:,1] = hsv[:,:,1] * np.random.uniform(0.7, 1.3)
    image = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)
    
    return image
```

**Benefit:**
- Simulate real-world variations (lighting, camera quality, skin tones)
- Model lebih robust terhadap kondisi berbeda

### 3. **Mixup Augmentation**

```python
def mixup(x1, y1, x2, y2, alpha=0.2):
    """Combine 2 samples"""
    lam = np.random.beta(alpha, alpha)
    x_mixed = lam * x1 + (1 - lam) * x2
    y_mixed = lam * y1 + (1 - lam) * y2
    return x_mixed, y_mixed
```

**Cara Kerja:**
- Blend 2 gambar penyakit berbeda
- Model belajar smooth transitions
- Regularization effect yang kuat

**Contoh:**
- 70% Acne + 30% Eczema ‚Üí label: [0.7, 0.3, 0, 0, ...]
- Force model untuk tidak overconfident

### 4. **Augmentation Factor: 15x**

```
Original: 11 foto/kelas √ó 10 kelas = 110 foto
After 15x augmentation: 165 foto/kelas √ó 10 kelas = 1,650 foto
```

**Kenapa 15x?**
- Untuk dataset 11 foto, idealnya butuh 200-500 foto/kelas
- 15x augmentation ‚Üí ~165 foto/kelas (mendekati minimal)
- Lebih dari 20x ‚Üí diminishing returns, training terlalu lama

---

## üîÑ K-FOLD CROSS VALIDATION: Mengapa Penting?

### Masalah dengan Simple Train/Val Split (Original Code):

```
110 foto total
‚îú‚îÄ‚îÄ 80% train = 88 foto (8-9 foto/kelas)
‚îî‚îÄ‚îÄ 20% val = 22 foto (2-3 foto/kelas)
```

**Masalah:**
- Validation set hanya 2-3 foto per kelas ‚Üí **TIDAK RELIABLE**
- Model hanya "lihat" 88 foto untuk training
- Estimasi akurasi sangat noisy

### Solusi: K-Fold Cross Validation (5-Fold)

```
Fold 1: Train[88 foto] ‚Üí Val[22 foto]
Fold 2: Train[88 foto] ‚Üí Val[22 foto] (different 22)
Fold 3: Train[88 foto] ‚Üí Val[22 foto]
Fold 4: Train[88 foto] ‚Üí Val[22 foto]
Fold 5: Train[88 foto] ‚Üí Val[22 foto]
```

**Benefit:**
1. ‚úÖ **Setiap foto** digunakan untuk validation sekali
2. ‚úÖ **Setiap foto** digunakan untuk training 4 kali
3. ‚úÖ Estimasi akurasi lebih reliable (average 5 folds)
4. ‚úÖ Dapat 5 models ‚Üí ensemble untuk prediksi lebih robust

**Final Accuracy:**
```
Fold accuracies: [0.85, 0.82, 0.88, 0.84, 0.86]
Mean: 0.85 ¬± 0.02  ‚Üê More reliable than single 0.85
```

---

## üîÆ TEST-TIME AUGMENTATION (TTA): Prediksi Lebih Stabil

### Masalah dengan Single Prediction:

```python
# Original: 1 prediction
prediction = model.predict(image)
# Result: [0.4, 0.6, 0, 0, ...]  ‚Üê Unstable!
```

Dengan foto tidak konsisten (angle berbeda), model bisa:
- Foto angle 1 ‚Üí Predict: Acne (60%)
- Foto angle 2 (same person) ‚Üí Predict: Rosacea (55%)

### Solusi: Test-Time Augmentation

```python
# TTA: 10 predictions on augmented versions
predictions = []
for _ in range(10):
    aug_image = augment(image)
    pred = model.predict(aug_image)
    predictions.append(pred)

# Average predictions
final_prediction = np.mean(predictions, axis=0)
# Result: [0.42, 0.58, 0, 0, ...]  ‚Üê More stable!
```

**Benefit:**
- Reduce variance dalam prediksi
- Lebih robust terhadap angle/lighting yang berbeda
- Confidence score lebih reliable

**Trade-off:**
- 10x lebih lama untuk predict
- Tapi untuk medical diagnosis, akurasi > speed

---

## üèóÔ∏è ARCHITECTURE IMPROVEMENTS

### 1. **Smaller Image Size: 96x96 (dari 128x128)**

**Kenapa Lebih Kecil?**

```
128x128 ‚Üí Base model output: ~4096 features
96x96 ‚Üí Base model output: ~2304 features

Fewer features ‚Üí Fewer parameters ‚Üí Less overfitting
```

**Trade-off:**
- ‚ùå Lose some detail
- ‚úÖ Much better for small dataset
- ‚úÖ Faster training
- ‚úÖ Less memory

### 2. **Heavy Regularization**

```python
# Original
Dense(512) ‚Üí Dropout(0.5) ‚Üí Dense(256) ‚Üí Dropout(0.4)

# Improved
Dense(256, L2_reg=0.01) ‚Üí BatchNorm ‚Üí Dropout(0.5) ‚Üí
Dense(128, L2_reg=0.01) ‚Üí BatchNorm ‚Üí Dropout(0.5)
```

**Changes:**
1. ‚úÖ Smaller layers (512‚Üí256, 256‚Üí128)
2. ‚úÖ L2 regularization (weight decay)
3. ‚úÖ More BatchNormalization
4. ‚úÖ Higher dropout (0.4‚Üí0.5)

**Effect:**
- Force model to learn simpler patterns
- Prevent memorization of training data

### 3. **Dual Global Pooling**

```python
# Original
x = GlobalAveragePooling2D()(base_output)

# Improved
gap = GlobalAveragePooling2D()(base_output)
gmp = GlobalMaxPooling2D()(base_output)
x = Concatenate([gap, gmp])
```

**Benefit:**
- GAP: Captures average spatial information
- GMP: Captures strongest activations
- Combined: More robust feature representation

---

## üé≠ ENSEMBLE PREDICTIONS

### Single Model vs Ensemble

```python
# Single Model (Original)
prediction = model.predict(test_image)
accuracy = 0.75

# Ensemble: 5 K-Fold Models + TTA (Improved)
predictions = []
for model in kfold_models:
    for _ in range(10):  # TTA
        aug_image = augment(test_image)
        pred = model.predict(aug_image)
        predictions.append(pred)

final_prediction = np.mean(predictions, axis=0)
accuracy = 0.82  # ‚Üê Improvement!
```

**Benefit:**
- Reduce model variance
- Reduce prediction variance
- More stable and reliable

---

## üì∏ MENGENAI FOTO YANG TIDAK KONSISTEN

### ‚ö†Ô∏è **RISIKO BESAR!**

Foto tidak konsisten (angle berbeda, fokus berbeda, dll):

**Contoh Masalah:**
```
Dataset Acne:
- 5 foto close-up hidung
- 3 foto full face dari depan
- 3 foto side profile

Model akan belajar:
"Acne = foto close-up hidung" ‚Üê SALAH!
```

**Akibatnya:**
- Model akan bias ke lokasi foto, bukan ke penyakit
- Prediksi tidak reliable pada angle berbeda
- Akurasi tampak tinggi di training, tapi gagal di real-world

### ‚úÖ **SOLUSI UNTUK FOTO TIDAK KONSISTEN**

#### 1. **Crop dan Standardize (HIGHLY RECOMMENDED)**

```python
def preprocess_inconsistent_photos(image_path):
    """
    Standardize photos dengan crop ke area yang penting
    """
    img = cv2.imread(image_path)
    
    # Detect face
    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface.xml')
    faces = face_cascade.detectMultiScale(img)
    
    if len(faces) > 0:
        # Crop ke face
        x, y, w, h = faces[0]
        img_cropped = img[y:y+h, x:x+w]
    else:
        # Fallback: center crop
        h, w = img.shape[:2]
        size = min(h, w)
        start_h = (h - size) // 2
        start_w = (w - size) // 2
        img_cropped = img[start_h:start_h+size, start_w:start_w+size]
    
    # Resize to standard size
    img_final = cv2.resize(img_cropped, (96, 96))
    
    return img_final
```

**Benefit:**
- Semua foto jadi konsisten (face crop)
- Model fokus ke area yang relevant
- Reduce bias dari angle/composition

#### 2. **Extreme Augmentation untuk Simulasi Angles**

Karena foto asli sudah tidak konsisten, augmentasi harus lebih agresif:

```python
augmentation = ImageDataGenerator(
    rotation_range=40,        # Simulate different angles
    width_shift_range=0.3,    # Simulate different compositions
    height_shift_range=0.3,
    zoom_range=0.4,           # Simulate different distances
    horizontal_flip=True,
    vertical_flip=True,
    # ... dll
)
```

**Logika:**
- Foto asli sudah random ‚Üí augmentation bikin lebih random lagi
- Force model untuk "tidak peduli" angle/composition
- Fokus ke fitur penyakit itu sendiri (texture, color, pattern)

#### 3. **Multi-Scale Training (Advanced)**

```python
# Train dengan berbagai ukuran input
for epoch in range(EPOCHS):
    for img_size in [80, 96, 112]:  # Multi-scale
        # Resize images
        X_resized = [cv2.resize(img, (img_size, img_size)) for img in X_train]
        # Train
        model.fit(X_resized, y_train, ...)
```

**Benefit:**
- Model belajar fitur di berbagai skala
- Lebih robust terhadap crop/zoom yang berbeda

#### 4. **Attention Mechanism (Advanced)**

Tambahkan attention layer untuk force model fokus ke area penyakit:

```python
from tensorflow.keras.layers import Multiply, Lambda

def attention_block(x):
    # Generate attention map
    attention = Conv2D(1, (1, 1), activation='sigmoid')(x)
    
    # Apply attention
    x = Multiply()([x, attention])
    
    return x

# Dalam model:
x = base_model(inputs)
x = attention_block(x)  # ‚Üê Add this
x = GlobalAveragePooling2D()(x)
```

**Benefit:**
- Model belajar untuk "look at" area yang penting
- Ignore irrelevant background/angle variations

---

## üìä EXPECTED PERFORMANCE

### Dengan Dataset 11 Foto/Kelas:

| Metode | Expected Accuracy | Keterangan |
|--------|-------------------|------------|
| **Original Code** | 40-60% | Severe overfitting, tidak reliable |
| **Improved Code** | **60-75%** | Dengan K-Fold + TTA + Heavy Aug |
| **+ Foto Konsisten** | **75-85%** | Standardize photos |
| **+ 50 Foto/Kelas** | **85-92%** | Collect more data |
| **+ 200 Foto/Kelas** | **92-97%** | Professional-level |

### ‚ö†Ô∏è **Reality Check:**

Dengan hanya 11 foto per kelas:
- ‚ùå Akurasi 90%+ ‚Üí **SANGAT SULIT**, kemungkinan besar overfit
- ‚úÖ Akurasi 70-75% ‚Üí **REALISTIC dan GOOD** untuk dataset kecil
- ‚úÖ K-Fold std < 5% ‚Üí **Model stable**

**Jangan tertipu akurasi tinggi!** Validation accuracy 95% tapi test accuracy 60% = overfitting!

---

## üöÄ ACTION PLAN: Step-by-Step

### IMMEDIATE (Can do now):

#### 1. **Standardize Photos** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (MOST IMPORTANT!)

```bash
# Create script to crop all photos consistently
python preprocess_photos.py --input dataset/raw --output dataset/standardized
```

**Guidelines:**
- ‚úÖ Crop ke face area saja (remove background)
- ‚úÖ Same angle (frontal face preferred)
- ‚úÖ Same lighting (normalize brightness)
- ‚úÖ Same distance (fill frame dengan face)
- ‚úÖ Remove glasses/accessories jika possible

#### 2. **Use Improved Code**

```bash
# Run improved training script
python skin_disease_classifier_IMPROVED.py
```

**Expected:**
- Training time: 2-4 hours (dengan K-Fold)
- Memory: ~4GB RAM, 2GB GPU (jika ada)
- Output: 5 models (K-Fold) + metadata

#### 3. **Monitor Overfitting Carefully**

```python
# Check K-Fold results
Fold 1: Val Acc = 0.75
Fold 2: Val Acc = 0.73
Fold 3: Val Acc = 0.77
Fold 4: Val Acc = 0.74
Fold 5: Val Acc = 0.76

Mean: 0.75 ¬± 0.015  ‚Üê Good! Low std = stable model

# Red flags:
Fold 1: Val Acc = 0.95  ‚Üê TOO HIGH
Fold 2: Val Acc = 0.60  ‚Üê Big variance
Fold 3: Val Acc = 0.98
‚Üí OVERFITTING! Need more regularization
```

### SHORT TERM (1-2 weeks):

#### 1. **Collect More Data** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

Target: **Minimal 50 foto per kelas**

**Sumber:**
- Dermnet (medical image database)
- Google Images (dengan proper filtering)
- Medical institutions collaboration
- Synthetic data generation (GAN)

**Important:**
- ‚úÖ Consistent quality
- ‚úÖ Proper labeling (konsultasi dermatologist)
- ‚úÖ Diverse skin tones
- ‚úÖ Various severity levels

#### 2. **Data Cleaning**

```python
# Remove outliers, duplicates, mislabeled
from sklearn.cluster import DBSCAN

# Extract features
features = model_feature_extractor.predict(X)

# Find outliers
outliers = DBSCAN(eps=0.5).fit_predict(features)

# Manual review outliers
review_images(X[outliers == -1])
```

#### 3. **Hyperparameter Tuning**

```python
# Try different configurations
configs = [
    {'img_size': 96, 'batch_size': 4, 'lr': 0.0001},
    {'img_size': 112, 'batch_size': 8, 'lr': 0.00005},
    {'img_size': 80, 'batch_size': 4, 'lr': 0.0002},
]

for config in configs:
    model = train_with_config(config)
    evaluate(model)
```

### LONG TERM (1-3 months):

#### 1. **Advanced Techniques**

- [ ] Semi-supervised learning (use unlabeled data)
- [ ] Transfer learning from similar medical domains
- [ ] Synthetic data generation (StyleGAN)
- [ ] Active learning (label most informative samples first)

#### 2. **Model Ensemble**

- [ ] Train multiple base models (MobileNet, EfficientNet, ResNet)
- [ ] Ensemble dengan voting/stacking
- [ ] Expected improvement: +3-5% accuracy

#### 3. **Domain-Specific Preprocessing**

- [ ] Skin color normalization
- [ ] Hair removal (jika menghalangi)
- [ ] Glare removal
- [ ] Contrast enhancement khusus untuk skin

---

## üìã CHECKLIST KUALITAS

### ‚úÖ **MUST DO:**

- [ ] Standardize semua foto (same angle, crop, lighting)
- [ ] Use K-Fold Cross Validation
- [ ] Enable heavy augmentation
- [ ] Use Test-Time Augmentation
- [ ] Monitor overfitting (val loss vs train loss)
- [ ] Test pada foto baru (bukan dari dataset)

### ‚ö†Ô∏è **SHOULD DO:**

- [ ] Collect more data (target: 50+ foto/kelas)
- [ ] Consult dermatologist untuk labeling
- [ ] Data cleaning (remove duplicates/outliers)
- [ ] Implement attention mechanism
- [ ] Try different base models

### üí° **NICE TO HAVE:**

- [ ] Synthetic data generation
- [ ] Multi-task learning (predict severity + type)
- [ ] Explainable AI (Grad-CAM visualization)
- [ ] Deploy as web API

---

## üéØ KESIMPULAN

### ‚ùå **JANGAN:**

1. **JANGAN** pakai foto tidak konsisten tanpa preprocessing
2. **JANGAN** percaya akurasi 95%+ dengan dataset 11 foto/kelas
3. **JANGAN** skip K-Fold CV untuk dataset kecil
4. **JANGAN** gunakan test set untuk tuning hyperparameter
5. **JANGAN** deploy model tanpa testing comprehensive

### ‚úÖ **LAKUKAN:**

1. **STANDARDIZE** foto (crop, angle, lighting)
2. **GUNAKAN** improved code dengan K-Fold + TTA
3. **COLLECT** lebih banyak data (target: 50-200/kelas)
4. **MONITOR** overfitting dengan validation curves
5. **TEST** pada data real-world sebelum deploy

### üéì **INGAT:**

> **"Garbage In, Garbage Out"**
> 
> Kualitas data > Kompleksitas model
> 11 foto berkualitas > 100 foto random
> Foto konsisten > Augmentation intensif

### üìä **Ekspektasi Realistis:**

Dengan improved code + foto konsisten:
- ‚úÖ Accuracy 70-80%: **GOOD untuk dataset kecil**
- ‚ö†Ô∏è Accuracy 60-70%: **Acceptable, perlu improvement**
- ‚ùå Accuracy <60%: **Problem dengan data atau model**

---

## üìû TROUBLESHOOTING

### Problem: "Validation accuracy 95%, test accuracy 50%"

**Diagnosis:** Severe overfitting

**Solution:**
1. Reduce model complexity (smaller dense layers)
2. Increase dropout (0.5 ‚Üí 0.6)
3. More aggressive augmentation
4. Check data leakage (apakah test data ada yang mirip training?)

### Problem: "Training accuracy stuck at 70%"

**Diagnosis:** Underfitting atau data quality issue

**Solution:**
1. Check data labels (apakah benar?)
2. Visualize mislabeled samples
3. Increase model capacity sedikit
4. Decrease regularization sedikit
5. Check data preprocessing (apakah ada yang corrupt?)

### Problem: "K-Fold variance sangat tinggi (std >10%)"

**Diagnosis:** Dataset terlalu kecil atau tidak balanced

**Solution:**
1. Collect more data
2. Use stratified split (pastikan setiap fold balanced)
3. Increase augmentation factor
4. Check outliers dalam data

---

## üìö RESOURCES

### Papers:
- Mixup: https://arxiv.org/abs/1710.09412
- Test-Time Augmentation: https://arxiv.org/abs/1904.07399
- EfficientNet: https://arxiv.org/abs/1905.11946

### Datasets:
- DermNet: https://www.dermnet.com/
- HAM10000: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T
- ISIC Archive: https://www.isic-archive.com/

### Tools:
- Label Studio (labeling): https://labelstud.io/
- Grad-CAM (visualization): https://github.com/jacobgil/pytorch-grad-cam
- Albumentations (augmentation): https://albumentations.ai/

---

**Good luck! üöÄ**

Jika ada pertanyaan atau butuh klarifikasi, jangan ragu untuk bertanya!
