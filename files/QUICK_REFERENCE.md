# ‚ö° QUICK REFERENCE: Original vs Improved

## üìã IMPLEMENTATION CHECKLIST

### Phase 1: IMMEDIATE (Hari ini)

- [ ] **STANDARDIZE PHOTOS** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
  ```bash
  python preprocess_photos.py --input ./raw --output ./processed --save-comparison
  ```
  - **WHY:** Inconsistent photos = confused model
  - **IMPACT:** +10-20% accuracy
  - **TIME:** 30 mins - 1 hour

- [ ] **RUN IMPROVED CODE**
  ```bash
  python skin_disease_classifier_IMPROVED.py
  ```
  - **WHY:** Heavy aug + K-Fold + TTA
  - **IMPACT:** +5-15% accuracy vs original
  - **TIME:** 2-4 hours training

- [ ] **REVIEW RESULTS**
  - Check K-Fold std deviation (should be <5%)
  - Check confusion matrix (which classes confused?)
  - Visualize misclassified samples
  - **TIME:** 30 mins

### Phase 2: SHORT TERM (Minggu ini)

- [ ] **DATA QUALITY AUDIT**
  - Manually review 10 random samples per class
  - Check labeling accuracy (konsultasi expert jika perlu)
  - Remove duplicates atau corrupted images
  - **TIME:** 2-3 hours

- [ ] **COLLECT MORE DATA** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
  - Target: 30-50 foto per kelas (total 300-500)
  - Sources: DermNet, HAM10000, medical partners
  - **IMPACT:** +15-25% accuracy
  - **TIME:** Ongoing

- [ ] **HYPERPARAMETER TUNING**
  - Try img_size: [80, 96, 112]
  - Try batch_size: [4, 8, 16]
  - Try learning_rate: [1e-4, 5e-5, 1e-5]
  - **TIME:** 1 day (parallel training)

### Phase 3: LONG TERM (Bulan ini)

- [ ] **ADVANCED TECHNIQUES**
  - Implement attention mechanism
  - Try ensemble of different base models
  - Experiment with Mixup/CutMix
  - **TIME:** 1-2 weeks

- [ ] **DEPLOYMENT PREP**
  - Convert to TFLite for mobile
  - Create REST API
  - Build simple web interface
  - **TIME:** 1 week

---

## üìä COMPARISON TABLE: What Changed?

| Component | Original | Improved | Impact | Priority |
|-----------|----------|----------|--------|----------|
| **Image Size** | 128√ó128 | **96√ó96** | Reduce overfitting | ‚≠ê‚≠ê‚≠ê |
| **Batch Size** | 32 | **4** | More updates per epoch | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Validation** | Simple split | **K-Fold (5)** | Every sample used | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Augmentation** | Medium | **Heavy (15x)** | Massive data increase | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Rotation** | ¬±20¬∞ | **¬±40¬∞** | Handle angle variations | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Flips** | Horizontal only | **Both H & V** | More variations | ‚≠ê‚≠ê‚≠ê |
| **Brightness** | [0.8, 1.2] | **[0.5, 1.5]** | Handle lighting | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Custom Aug** | None | **Noise + Saturation** | Realistic variations | ‚≠ê‚≠ê‚≠ê |
| **Mixup** | ‚ùå No | **‚úÖ Yes** | Smooth boundaries | ‚≠ê‚≠ê‚≠ê |
| **TTA** | ‚ùå No | **‚úÖ Yes (10x)** | Stable predictions | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Ensemble** | Single model | **5 models** | Reduce variance | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Dense Layers** | 512-256-128 | **256-128** | Fewer parameters | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Dropout** | 0.5, 0.4, 0.3 | **0.5, 0.5, 0.4** | More regularization | ‚≠ê‚≠ê‚≠ê |
| **L2 Reg** | ‚ùå No | **‚úÖ 0.01** | Weight decay | ‚≠ê‚≠ê‚≠ê |
| **BatchNorm** | After each dense | **After EVERY layer** | Stable training | ‚≠ê‚≠ê‚≠ê |
| **Pooling** | GAP only | **GAP + GMP** | Better features | ‚≠ê‚≠ê |
| **Early Stop** | Patience 15 | **Patience 20** | Don't stop too early | ‚≠ê‚≠ê |
| **LR Reduce** | Patience 7 | **Patience 10** | More chances | ‚≠ê‚≠ê |

**Legend:**
- ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê = CRITICAL (must have for small dataset)
- ‚≠ê‚≠ê‚≠ê‚≠ê = Very Important
- ‚≠ê‚≠ê‚≠ê = Important
- ‚≠ê‚≠ê = Nice to have

---

## üéØ EXPECTED RESULTS

### Scenario 1: Original Code + Inconsistent Photos (11/class)

```
Training accuracy: 85-95%  ‚Üê Looks good!
Validation accuracy: 60-70%  ‚Üê Big gap = overfitting
Test accuracy: 50-60%  ‚Üê POOR on new data
K-Fold std: N/A

üö® PROBLEM: Severe overfitting, unreliable predictions
```

### Scenario 2: Improved Code + Inconsistent Photos (11/class)

```
K-Fold Results:
  Fold 1: 68%
  Fold 2: 72%
  Fold 3: 70%
  Fold 4: 69%
  Fold 5: 71%
  Mean: 70% ¬± 1.5%  ‚Üê Low std = good!

Test accuracy (with TTA): 68-72%

‚úÖ BETTER: More stable, reliable predictions
‚ö†Ô∏è  Still limited by small dataset
```

### Scenario 3: Improved Code + Standardized Photos (11/class)

```
K-Fold Results:
  Fold 1: 74%
  Fold 2: 77%
  Fold 3: 76%
  Fold 4: 75%
  Fold 5: 78%
  Mean: 76% ¬± 1.5%

Test accuracy (with TTA): 74-78%

‚úÖ GOOD: Consistent quality helps a lot!
üí° Ready for production (with caveats)
```

### Scenario 4: Improved Code + Standardized Photos (50/class)

```
K-Fold Results:
  Fold 1: 84%
  Fold 2: 86%
  Fold 3: 85%
  Fold 4: 83%
  Fold 5: 87%
  Mean: 85% ¬± 1.5%

Test accuracy (with TTA): 83-87%

‚úÖ EXCELLENT: Ready for production
üéØ Professional-level accuracy
```

---

## üîç HOW TO IDENTIFY PROBLEMS

### ‚úÖ HEALTHY TRAINING

```python
# Training curves should look like:
Epoch 10: train_loss=0.8, val_loss=0.9  ‚Üê Small gap
Epoch 20: train_loss=0.5, val_loss=0.6
Epoch 30: train_loss=0.3, val_loss=0.4
Epoch 40: train_loss=0.2, val_loss=0.3  ‚Üê Gap stays small

# K-Fold results:
Mean: 75% ¬± 2%  ‚Üê Low variance
```

**Signs:**
- Train and val loss decrease together
- Small gap between train and val (10-15%)
- Low K-Fold variance (<5%)
- Test accuracy close to validation accuracy

### üö® OVERFITTING

```python
# RED FLAGS:
Epoch 10: train_loss=0.8, val_loss=0.9
Epoch 20: train_loss=0.3, val_loss=0.7  ‚Üê Gap widening!
Epoch 30: train_loss=0.1, val_loss=0.9  ‚Üê BAD!
Epoch 40: train_loss=0.01, val_loss=1.2  ‚Üê WORSE!

# K-Fold results:
Mean: 75% ¬± 15%  ‚Üê High variance!
```

**Signs:**
- Train loss keeps decreasing, val loss increases
- Huge gap between train and val (>30%)
- High K-Fold variance (>10%)
- Test accuracy much lower than validation

**Solutions:**
1. Increase dropout (0.5 ‚Üí 0.6)
2. Add more L2 regularization (0.01 ‚Üí 0.02)
3. Reduce model size (256‚Üí128, 128‚Üí64)
4. More augmentation
5. Collect more data

### üêå UNDERFITTING

```python
# Symptoms:
Epoch 50: train_loss=0.5, val_loss=0.5  ‚Üê Not decreasing
Train accuracy: 65%
Val accuracy: 63%  ‚Üê Both low

# K-Fold results:
Mean: 60% ¬± 2%  ‚Üê Consistently low
```

**Signs:**
- Both train and val accuracy low
- Losses plateau early
- Can't learn patterns

**Solutions:**
1. Check data quality (labeling correct?)
2. Increase model capacity
3. Decrease regularization
4. Train longer
5. Check preprocessing (images corrupt?)

---

## üì∏ PHOTO CONSISTENCY GUIDE

### ‚ùå BAD DATASET (Don't do this!)

```
Acne Class:
‚îú‚îÄ‚îÄ IMG_001.jpg  (close-up nose, 300x400)
‚îú‚îÄ‚îÄ IMG_002.jpg  (full face far away, 800x600)
‚îú‚îÄ‚îÄ IMG_003.jpg  (side profile, 500x700)
‚îú‚îÄ‚îÄ IMG_004.jpg  (only forehead, 200x200)
‚îî‚îÄ‚îÄ IMG_005.jpg  (with glasses, dark lighting, 1024x768)

üö® PROBLEMS:
- Different angles ‚Üí model confused
- Different distances ‚Üí model learns "close-up = acne"
- Different lighting ‚Üí color features unreliable
- Accessories ‚Üí model might learn "glasses = acne"
```

### ‚úÖ GOOD DATASET (Do this!)

```
Acne Class:
‚îú‚îÄ‚îÄ acne_001.jpg  (frontal face, 96x96, standardized lighting)
‚îú‚îÄ‚îÄ acne_002.jpg  (frontal face, 96x96, standardized lighting)
‚îú‚îÄ‚îÄ acne_003.jpg  (frontal face, 96x96, standardized lighting)
‚îú‚îÄ‚îÄ acne_004.jpg  (frontal face, 96x96, standardized lighting)
‚îî‚îÄ‚îÄ acne_005.jpg  (frontal face, 96x96, standardized lighting)

‚úÖ CONSISTENCY:
- Same angle (frontal)
- Same size (96x96)
- Same crop (face only, no extra background)
- Same lighting (normalized)
- No accessories
```

### üõ†Ô∏è HOW TO STANDARDIZE

Use the preprocessing script:

```bash
# Step 1: Preprocess with face detection
python preprocess_photos.py \
    --input ./dataset/raw \
    --output ./dataset/standardized \
    --size 96 \
    --save-comparison

# Step 2: Manually review comparison images
ls ./dataset/standardized/*_comparison.jpg

# Step 3: Remove failed/low-quality images
# Check: ./dataset/standardized/failed_images.json

# Step 4: Train with standardized dataset
python skin_disease_classifier_IMPROVED.py
```

---

## üí° QUICK TIPS

### For 11 photos/class:

1. ‚úÖ **MUST DO:**
   - Standardize photos
   - Use K-Fold (5 folds)
   - Heavy augmentation (15x)
   - Test-Time Augmentation

2. ‚ö†Ô∏è **DON'T:**
   - Trust validation accuracy >85% (likely overfitting)
   - Use large models (ResNet152, EfficientNetB7)
   - Train without early stopping
   - Deploy without comprehensive testing

3. üéØ **REALISTIC EXPECTATIONS:**
   - 70-80% accuracy = GOOD
   - 60-70% accuracy = OK, needs improvement
   - <60% accuracy = Problem with data or model
   - >85% accuracy = Suspicious, check overfitting

### For 50 photos/class:

1. ‚úÖ **RECOMMENDED:**
   - K-Fold (5-10 folds)
   - Moderate augmentation (10x)
   - Test-Time Augmentation
   - Try larger models (EfficientNetB0-B3)

2. üéØ **EXPECTATIONS:**
   - 80-90% accuracy = GOOD
   - 70-80% accuracy = OK
   - >90% accuracy = Possible, verify carefully

### For 200+ photos/class:

1. ‚úÖ **OPTIMAL:**
   - Simple train/val/test split is OK
   - Moderate augmentation (5x)
   - Larger models possible
   - Can try advanced techniques

2. üéØ **EXPECTATIONS:**
   - 85-95% accuracy = GOOD
   - >95% accuracy = Excellent!

---

## üêõ DEBUGGING GUIDE

### Issue: "All predictions are Class 0"

**Diagnosis:** Class imbalance atau broken model

**Check:**
```python
# Check class distribution
Counter(y_train)
# Should be roughly balanced

# Check if model actually learning
print(model.predict(X_train[:10]))
# Should NOT be all same class
```

**Fix:**
- Use class weights
- Check data loading (labels correct?)
- Increase learning rate temporarily

### Issue: "Loss = NaN"

**Diagnosis:** Numerical instability

**Fix:**
- Reduce learning rate (1e-4 ‚Üí 1e-5)
- Check data normalization (should be [0,1])
- Remove extreme augmentation
- Add gradient clipping

### Issue: "Training very slow"

**Diagnosis:** Too much augmentation or large batch

**Fix:**
- Reduce augmentation factor (15x ‚Üí 10x)
- Disable TTA during training (only use for test)
- Use GPU if available
- Reduce image size (96 ‚Üí 80)

### Issue: "High variance in K-Fold"

**Diagnosis:** Dataset too small or imbalanced

**Fix:**
- Collect more data
- Use stratified K-Fold (already implemented)
- Increase augmentation
- Check for outliers

---

## ‚úÖ FINAL CHECKLIST

Before deployment, verify:

- [ ] Training completed without errors
- [ ] K-Fold std < 5%
- [ ] Test accuracy within 5% of validation accuracy
- [ ] Confusion matrix: no class completely failing
- [ ] Test on completely new photos (not from dataset)
- [ ] Visualize predictions with Grad-CAM (explain predictions)
- [ ] Document known limitations
- [ ] Set appropriate confidence thresholds (e.g., only predict if >80% confidence)

---

## üìû WHEN TO ASK FOR HELP

üö® **Red Flags:**

1. Test accuracy <50% after all improvements
2. K-Fold variance >15%
3. Training loss not decreasing after 10 epochs
4. Validation loss exploding
5. All predictions same class
6. Model predicting random on validation set

üí° **Next Steps:**

1. Double-check data (labels, quality)
2. Try simplest possible model first
3. Visualize what model is learning (Grad-CAM)
4. Consult with medical expert on labeling
5. Consider if problem is actually solvable with current data

---

**Remember:** 
> With only 11 photos/class, achieving 70-80% accuracy is GOOD.
> Don't chase unrealistic numbers - focus on reliable, consistent performance.

Good luck! üöÄ
